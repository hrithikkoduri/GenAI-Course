{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "## LLM\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "## Pinecone Vector Database\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrithikkoduri/Desktop/Course/venv/lib/python3.13/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"rag-multi-query-rag-fusion-index\" # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from pprint import pprint\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Document (Uploading one file at a time)\n",
    "pdf_file_path = \"./data/langchain_turing.pdf\"\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Upload muiltiple PDF files from a directory\n",
    "# pdf_file_paths = <enter your path here>\n",
    "# loader = PyPDFDirectoryLoader(pdf_file_paths)\n",
    "\n",
    "# docs_dir = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=500)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index\n",
    "vectorstore = Pinecone.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
    "    index_name=index_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG Fusion](./images/multi_query_rag_fusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link - https://medium.com/towards-data-science/forget-rag-the-future-is-rag-fusion-1147298d8ad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Question(BaseModel):\n",
    "    generated_questions: list[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def generate_questions(question):\n",
    "    template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "    Generate multiple search queries related to: {question} \\n\n",
    "    Output (4 queries):\"\"\"\n",
    "    prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "    generated_questions_prompt = prompt_rag_fusion.invoke(\n",
    "        {\"question\": question}\n",
    "    )\n",
    "    llm_with_structured_output = llm.with_structured_output(Question)\n",
    "\n",
    "\n",
    "    generated_questions = llm_with_structured_output.invoke(generated_questions_prompt)\n",
    "\n",
    "    return generated_questions.generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_retriever = generate_questions | retriever.map()\n",
    "\n",
    "question = \"How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?\"\n",
    "\n",
    "all_docs = all_docs_retriever.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 5.0, 'page_label': '6', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='6 Vasilios Mavroudis\\n1.5 Advanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2 LangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1 Tracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2 Performance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 0.0, 'page_label': '1', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe,andLangSmith.Weexplorehowtheframeworkfacilitatesthe\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models· LLM Applications·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,')],\n",
       " [Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 9.0, 'page_label': '10', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='10 Vasilios Mavroudis\\n3.4 Integration with LangChain and LangSmith\\nLangGraph integrates seamlessly with LangChain and LangSmith, although\\nit operates independently if desired. For users within the LangChain ecosys-\\ntem, LangGraph can utilize LangSmith’s tracing capabilities to monitor each\\nnode’s performance and capture detailed logs of agent interactions. Addition-\\nally, LangChain tools and APIs can be incorporated as graph nodes, expanding\\nLangGraph’s functionality with LangChain’s extensive library of tools and con-\\nnectors [7].\\n4 LangServe\\nLangServe [5] is an integral component of the LangChain ecosystem, specifically\\ndesignedtofacilitatethedeploymentoflargelanguagemodel(LLM)applications\\nas scalable REST APIs. With LangServe, developers can create production-\\ngrade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with\\nrobust support for load balancing, monitoring, and scalability.\\n4.1 Core Features of LangServe\\nAPI Deployment and Management LangServe simplifies the process of\\nturning LangChain applications into APIs, making LLM models accessible for\\nvarious services and client applications. With LangServe, any LangChain work-\\nflow can be packaged and exposed via RESTful endpoints, enabling interactions\\nwith language models, data retrieval, and external tool integration. The API-\\ncentric design allows LangServe to support diverse use cases, from chatbots and\\nrecommendation systems to complex multi-agent interactions. It provides several\\ntools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint\\nbased on application requirements. This flexibility allows developers to design\\nAPIs with specific functionalities, making LangServe suitable for applications of\\nvarying complexity [5].\\nScalability and Load BalancingLangServe includes built-in support for scal-\\nability, making it ideal for applications that experience high traffic volumes. It\\ncan handle multiple API requests simultaneously, ensuring consistent perfor-\\nmance by balancing the load across instances. This feature is critical for pro-\\nduction environments where maintaining low response times under heavy loads\\nis essential. To further enhance scalability, LangServe provides tools for setting\\nup auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation\\nin performance, making LangServe suitable for real-world deployments with un-\\npredictable usage patterns [5].'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 0.0, 'page_label': '1', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe,andLangSmith.Weexplorehowtheframeworkfacilitatesthe\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models· LLM Applications·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.')],\n",
       " [Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 12.0, 'page_label': '13', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 13\\nLangChain’s security model addresses many of these concerns, yet challenges\\npersist, particularly in sectors with rigorous compliance standards, such as fi-\\nnance and healthcare. Key areas for ongoing improvement include:\\n– DynamicPermissionAdjustment :CurrentpermissionsettingsinLangChain\\nare defined at deployment, but in dynamic applications, permissions may\\nneed to adapt based on user interactions. Implementing adaptive permis-\\nsions responsive to application state or user roles could enhance security.\\n– Advanced Encryption Standards: For applications processing highly\\nsensitive data, adopting advanced encryption practices—such as end-to-end\\nor field-level encryption—could bolster data security within even trusted\\nenvironments.\\n– Proactive Security Analytics: Integrating predictive analytics to pre-\\nemptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative\\nof potential breaches or misuse.\\nIn summary, LangChain’s security framework includes robust features such\\nas granular permissions, sandboxing, and real-time monitoring. While these mea-\\nsures provide a solid foundation, the ongoing challenge of securing LLM-driven\\napplications—particularly those relying on external providers—demands contin-\\nued advancements in security practices.\\n6 Conclusion\\nLangChain significantly advances the development of applications powered by\\nlarge language models (LLMs). Its modular framework—including components\\nlike LangGraph for stateful process modeling, LangServe for scalable API de-\\nployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-\\nments in usability and security, LangChain can continue to serve as a valuable\\ntool for developers seeking to leverage LLMs in building innovative and secure\\napplications. As industries increasingly adopt AI technologies, frameworks like\\nLangChain are poised to play a pivotal role in shaping the next generation of\\nintelligent, scalable, and secure solutions across various sectors.'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 5.0, 'page_label': '6', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='6 Vasilios Mavroudis\\n1.5 Advanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2 LangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1 Tracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2 Performance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 11.0, 'page_label': '12', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='12 Vasilios Mavroudis\\nof the API. LangChain workflows, tools, and chains can be directly exposed\\nvia LangServe endpoints, providing flexible API interactions and enhancing the\\nfunctionality of LLM applications [5].\\n5 Limitations and Criticisms\\nLangChain provides a versatile framework for the development of applications\\npowered by large language models (LLMs). However, several limitations warrant\\nattention, especially in the domains of complexity and security.\\n5.1 Complexity\\nLangChain’s modular architecture, while designed to simplify LLM-based ap-\\nplication development, can paradoxically increase complexity. Effective use of\\nLangChain often requires a nuanced understanding of its distinct components,\\nsuch as LangGraph and LangSmith, as well as familiarity with its API ecosys-\\ntem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-\\nedge of each module may present a barrier to new users, complicating onboarding\\nand initial implementation phases.\\n5.2 Security Concerns\\nGiven LangChain’s modular design and extensive reliance on external integra-\\ntions, security presents a notable challenge. Although LangChain incorporates a\\nrange of security measures, including fine-grained permission control and sand-\\nboxing, the complexity of securing LLM-based applications remains high, espe-\\ncially for applications managing sensitive data. Below, we outline several critical\\nsecurity risks associated with LangChain and explore strategies for risk mitiga-\\ntion.\\nRisksAssociatedwithExternalProviders Toenhancefunctionality,LangChain\\nintegrateswithnumerousexternalservices,suchasvectordatabases,APIproviders,\\nand cloud storage platforms. However, these integrations expose applications to\\nsecurity vulnerabilities:\\n– Data Exposure: Accessing external resources can inadvertently expose sen-\\nsitive data to third-party providers, a risk particularly relevant for applica-\\ntions handling personal or confidential information. Without stringent data\\nencryption and access control mechanisms, the potential for data leaks or\\nunauthorized access increases.\\n– Third-Party Dependency: Reliance on third-party services introduces de-\\npendencies on their security protocols. Any compromise within a provider’s\\ninfrastructurecouldaffectLangChainapplications,resultingindatabreaches\\nor service interruptions. This underscores the importance of thoroughly vet-\\nting providers and monitoring them for potential security issues.'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 6.0, 'page_label': '7', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 7\\ncases, including inputs and expected outputs. Using these datasets, developers\\ncan conduct performance tests and assess how well their models meet expected\\noutcomes—an essential step for applications where accuracy and reliability are\\ncrucial. LangSmith supports custom evaluators, allowing developers to specify\\nscoring functions based on specific needs. For instance, an evaluator may mea-\\nsure the exact match between outputs and expected answers, or use metrics\\nlike cosine similarity for open-ended tasks. By supporting both built-in and cus-\\ntom evaluators, LangSmith provides flexibility in performance measurement for\\ndeterministic outputs or nuanced language generation tasks [9].\\n2.3 Dataset Management\\nDatasets are foundational to LangSmith’s evaluation system. Organizing test\\ncases into structured datasets enables methodical testing and validation. De-\\nvelopers can create datasets manually or import them from existing sources,\\nallowing diverse testing scenarios that reflect real-world use cases. Datasets can\\ncontain structured or unstructured data evaluations, accommodating a variety of\\ntesting needs. LangSmith’s dataset version control allows developers to maintain\\nmultiple dataset versions as applications evolve. This feature is critical for ensur-\\ning consistency in evaluation, especially as application logic changes or models\\nare retrained, providing a robust foundation for testing and validation [9].\\n2.4 LangSmith Workflow\\nLangSmith integrates tracing, evaluation, and dataset management into a cohe-\\nsive framework, enabling developers to progress from debugging to optimization\\nin a structured manner. A typical LangSmith workflow includes the following\\nstages:\\n– Trace Logging: Developers activate tracing on application functions, pro-\\nviding insights into model-component interactions.\\n– Dataset Creation and Evaluation: Developers create datasets represent-\\ning different scenarios to conduct comprehensive testing.\\n– Evaluation and Iterative Optimization: Evaluation results indicate per-\\nformance areas for refinement, guiding iterative application improvements.\\n– Version Control and Historical Tracking: LangSmith logs all interac-\\ntions, dataset versions, and evaluation scores, allowing developers to assess\\nimprovements over time.\\n2.5 Integration with LangChain and LangServe\\nLangSmith integrates seamlessly with LangChain and LangServe (Section 4) to\\nenhancetheend-to-endLLMapplicationdevelopmentexperience.ForLangChain\\nusers, LangSmith can automatically log traces and integrate with existing work-\\nflows. Combined with LangServe, LangSmith provides robust observability for\\nAPI deployments, monitoring real-time usage patterns, tracking request laten-\\ncies, and identifying bottlenecks.')],\n",
       " [Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 10.0, 'page_label': '11', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 11\\nLatency and Error ManagementLangServe is designed to minimize latency,\\nensuring quick responses for each API request. It employs efficient request queu-\\ning and processing mechanisms to reduce waiting times. Additionally, LangServe\\nincludes error handling and retry logic, which helps maintain reliability by man-\\naging transient failures and minimizing downtime. This focus on latency and\\nerror resilience makes LangServe suitable for mission-critical applications that\\nrequire high availability. For instance, LangServe can automatically retry failed\\nrequests and log errors for further investigation, allowing developers to identify\\nissues promptly and maintain a stable API response rate [5].\\n4.2 LangServe Workflow\\nThe LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:\\n1. Defining Endpoints: Developers define endpoints based on application re-\\nquirements, specifying which functions or models are exposed via API routes.\\nEach endpoint can have customized parameters, allowing for flexibility in\\nhow the API interacts with different components.\\n2. ConfiguringRequestHandlingandRouting :LangServeallowsforfine-\\ngrained control over how requests are processed. Developers can set up rout-\\ning rules, parameter validation, and request parsing to tailor the API expe-\\nrience.\\n3. Setting Up Load Balancing and Scaling: For applications with high\\ntraffic, LangServe’s load balancing can be configured to distribute requests\\nacross multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.\\n4. Monitoring and Error Tracking: LangServe integrates with monitoring\\ntools, including LangSmith, to provide real-time insights into API perfor-\\nmance, usage metrics, and error rates. This monitoring helps developers\\nmaintain optimal performance and quickly resolve issues as they arise.\\nLangServe’s streamlined workflow ensures that developers can deploy robust\\nAPIs with minimal overhead, making it a practical choice for scaling LLM ap-\\nplications in production environments.\\n4.3 Integration with LangSmith and LangChain\\nLangServe integrates seamlessly with LangSmith, which offers observability fea-\\ntures like tracing, logging, and performance monitoring. Through LangSmith,\\nLangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,\\nenabling developers to optimize applications based on real-time data.\\nAdditionally, LangServe’s integration with LangChain allows developers to\\nleverage LangChain’s extensive library of tools, models, and connectors as part'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 9.0, 'page_label': '10', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='10 Vasilios Mavroudis\\n3.4 Integration with LangChain and LangSmith\\nLangGraph integrates seamlessly with LangChain and LangSmith, although\\nit operates independently if desired. For users within the LangChain ecosys-\\ntem, LangGraph can utilize LangSmith’s tracing capabilities to monitor each\\nnode’s performance and capture detailed logs of agent interactions. Addition-\\nally, LangChain tools and APIs can be incorporated as graph nodes, expanding\\nLangGraph’s functionality with LangChain’s extensive library of tools and con-\\nnectors [7].\\n4 LangServe\\nLangServe [5] is an integral component of the LangChain ecosystem, specifically\\ndesignedtofacilitatethedeploymentoflargelanguagemodel(LLM)applications\\nas scalable REST APIs. With LangServe, developers can create production-\\ngrade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with\\nrobust support for load balancing, monitoring, and scalability.\\n4.1 Core Features of LangServe\\nAPI Deployment and Management LangServe simplifies the process of\\nturning LangChain applications into APIs, making LLM models accessible for\\nvarious services and client applications. With LangServe, any LangChain work-\\nflow can be packaged and exposed via RESTful endpoints, enabling interactions\\nwith language models, data retrieval, and external tool integration. The API-\\ncentric design allows LangServe to support diverse use cases, from chatbots and\\nrecommendation systems to complex multi-agent interactions. It provides several\\ntools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint\\nbased on application requirements. This flexibility allows developers to design\\nAPIs with specific functionalities, making LangServe suitable for applications of\\nvarying complexity [5].\\nScalability and Load BalancingLangServe includes built-in support for scal-\\nability, making it ideal for applications that experience high traffic volumes. It\\ncan handle multiple API requests simultaneously, ensuring consistent perfor-\\nmance by balancing the load across instances. This feature is critical for pro-\\nduction environments where maintaining low response times under heavy loads\\nis essential. To further enhance scalability, LangServe provides tools for setting\\nup auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation\\nin performance, making LangServe suitable for real-world deployments with un-\\npredictable usage patterns [5].'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 5.0, 'page_label': '6', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='6 Vasilios Mavroudis\\n1.5 Advanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2 LangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1 Tracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2 Performance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test'),\n",
       "  Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RRF?\n",
    "\n",
    "Reciprocal Rank Fusion is a method for combining multiple ranked lists into a single, improved ranked list. It's particularly useful in RAG (Retrieval Augmented Generation) systems when you have multiple ways of retrieving relevant documents and want to combine their results optimally.\n",
    "\n",
    "How the code works:\n",
    "\n",
    "### 1. Function Input:\n",
    "- results: A list of lists, where each inner list contains ranked documents\n",
    "- k: A constant (default=60) that acts as a smoothing factor to prevent high rankings from dominating the final score\n",
    "```bash\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "```\n",
    "\n",
    "### 2. Score Calculation:\n",
    "\n",
    "The core formula is: score = 1 / (rank + k)\n",
    "\n",
    "For example, if k=60:\n",
    "- Document at rank 0 gets score: 1/60 = 0.0167\n",
    "- Document at rank 1 gets score: 1/61 ≈ 0.0164\n",
    "- Document at rank 2 gets score: 1/62 ≈ 0.0161\n",
    "\n",
    "And so on...\n",
    "```bash\n",
    "fused_scores = {}\n",
    "for docs in results:\n",
    "    for rank, doc in enumerate(docs):\n",
    "        doc_str = dumps(doc)\n",
    "        if doc_str not in fused_scores:\n",
    "            fused_scores[doc_str] = 0\n",
    "        fused_scores[doc_str] += 1 / (rank + k)\n",
    "```\n",
    "\n",
    "### 3. Process Flow:\n",
    "- For each ranked list in the input:\n",
    "    - For each document in the list:\n",
    "        - Convert document to string (for dictionary key)\n",
    "        - Add its reciprocal rank score to any existing score\n",
    "- Documents appearing in multiple lists accumulate scores\n",
    "\n",
    "### 4. Final Reranking:\n",
    "```bash\n",
    "reranked_results = [\n",
    "    (loads(doc), score)\n",
    "    for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "]\n",
    "```\n",
    "\n",
    "- Sorts documents by their accumulated scores\n",
    "- Converts document strings back to their original format\n",
    "- Returns list of (document, score) tuples in descending order\n",
    "\n",
    "\n",
    "**Example:** \n",
    "\n",
    "List 1: [Doc A, Doc B, Doc C]\n",
    "\n",
    "List 2: [Doc B, Doc C, Doc A]\n",
    "\n",
    "**After RRF:**\n",
    "\n",
    "Doc A: 1/60 (from List 1) + 1/62 (from List 2) = 0.0334\n",
    "\n",
    "Doc B: 1/61 (from List 1) + 1/60 (from List 2) = 0.0331\n",
    "\n",
    "Doc C: 1/62 (from List 1) + 1/61 (from List 2) = 0.0325\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] = previous_score +  (1 / (rank + k))\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    sorted_items = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    reranked_results = [(loads(doc), score) for doc, score in sorted_items]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_docs = reciprocal_rank_fusion(all_docs, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       "  0.06504494976203068),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 5.0, 'page_label': '6', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='6 Vasilios Mavroudis\\n1.5 Advanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2 LangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1 Tracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2 Performance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test'),\n",
       "  0.04813947436898257),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'),\n",
       "  0.04791666666666666),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 9.0, 'page_label': '10', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='10 Vasilios Mavroudis\\n3.4 Integration with LangChain and LangSmith\\nLangGraph integrates seamlessly with LangChain and LangSmith, although\\nit operates independently if desired. For users within the LangChain ecosys-\\ntem, LangGraph can utilize LangSmith’s tracing capabilities to monitor each\\nnode’s performance and capture detailed logs of agent interactions. Addition-\\nally, LangChain tools and APIs can be incorporated as graph nodes, expanding\\nLangGraph’s functionality with LangChain’s extensive library of tools and con-\\nnectors [7].\\n4 LangServe\\nLangServe [5] is an integral component of the LangChain ecosystem, specifically\\ndesignedtofacilitatethedeploymentoflargelanguagemodel(LLM)applications\\nas scalable REST APIs. With LangServe, developers can create production-\\ngrade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with\\nrobust support for load balancing, monitoring, and scalability.\\n4.1 Core Features of LangServe\\nAPI Deployment and Management LangServe simplifies the process of\\nturning LangChain applications into APIs, making LLM models accessible for\\nvarious services and client applications. With LangServe, any LangChain work-\\nflow can be packaged and exposed via RESTful endpoints, enabling interactions\\nwith language models, data retrieval, and external tool integration. The API-\\ncentric design allows LangServe to support diverse use cases, from chatbots and\\nrecommendation systems to complex multi-agent interactions. It provides several\\ntools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint\\nbased on application requirements. This flexibility allows developers to design\\nAPIs with specific functionalities, making LangServe suitable for applications of\\nvarying complexity [5].\\nScalability and Load BalancingLangServe includes built-in support for scal-\\nability, making it ideal for applications that experience high traffic volumes. It\\ncan handle multiple API requests simultaneously, ensuring consistent perfor-\\nmance by balancing the load across instances. This feature is critical for pro-\\nduction environments where maintaining low response times under heavy loads\\nis essential. To further enhance scalability, LangServe provides tools for setting\\nup auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation\\nin performance, making LangServe suitable for real-world deployments with un-\\npredictable usage patterns [5].'),\n",
       "  0.03306010928961749),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'),\n",
       "  0.03200204813108039),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 0.0, 'page_label': '1', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe,andLangSmith.Weexplorehowtheframeworkfacilitatesthe\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models· LLM Applications·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,'),\n",
       "  0.031754032258064516),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 12.0, 'page_label': '13', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 13\\nLangChain’s security model addresses many of these concerns, yet challenges\\npersist, particularly in sectors with rigorous compliance standards, such as fi-\\nnance and healthcare. Key areas for ongoing improvement include:\\n– DynamicPermissionAdjustment :CurrentpermissionsettingsinLangChain\\nare defined at deployment, but in dynamic applications, permissions may\\nneed to adapt based on user interactions. Implementing adaptive permis-\\nsions responsive to application state or user roles could enhance security.\\n– Advanced Encryption Standards: For applications processing highly\\nsensitive data, adopting advanced encryption practices—such as end-to-end\\nor field-level encryption—could bolster data security within even trusted\\nenvironments.\\n– Proactive Security Analytics: Integrating predictive analytics to pre-\\nemptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative\\nof potential breaches or misuse.\\nIn summary, LangChain’s security framework includes robust features such\\nas granular permissions, sandboxing, and real-time monitoring. While these mea-\\nsures provide a solid foundation, the ongoing challenge of securing LLM-driven\\napplications—particularly those relying on external providers—demands contin-\\nued advancements in security practices.\\n6 Conclusion\\nLangChain significantly advances the development of applications powered by\\nlarge language models (LLMs). Its modular framework—including components\\nlike LangGraph for stateful process modeling, LangServe for scalable API de-\\nployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-\\nments in usability and security, LangChain can continue to serve as a valuable\\ntool for developers seeking to leverage LLMs in building innovative and secure\\napplications. As industries increasingly adopt AI technologies, frameworks like\\nLangChain are poised to play a pivotal role in shaping the next generation of\\nintelligent, scalable, and secure solutions across various sectors.'),\n",
       "  0.016666666666666666),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 10.0, 'page_label': '11', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 11\\nLatency and Error ManagementLangServe is designed to minimize latency,\\nensuring quick responses for each API request. It employs efficient request queu-\\ning and processing mechanisms to reduce waiting times. Additionally, LangServe\\nincludes error handling and retry logic, which helps maintain reliability by man-\\naging transient failures and minimizing downtime. This focus on latency and\\nerror resilience makes LangServe suitable for mission-critical applications that\\nrequire high availability. For instance, LangServe can automatically retry failed\\nrequests and log errors for further investigation, allowing developers to identify\\nissues promptly and maintain a stable API response rate [5].\\n4.2 LangServe Workflow\\nThe LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:\\n1. Defining Endpoints: Developers define endpoints based on application re-\\nquirements, specifying which functions or models are exposed via API routes.\\nEach endpoint can have customized parameters, allowing for flexibility in\\nhow the API interacts with different components.\\n2. ConfiguringRequestHandlingandRouting :LangServeallowsforfine-\\ngrained control over how requests are processed. Developers can set up rout-\\ning rules, parameter validation, and request parsing to tailor the API expe-\\nrience.\\n3. Setting Up Load Balancing and Scaling: For applications with high\\ntraffic, LangServe’s load balancing can be configured to distribute requests\\nacross multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.\\n4. Monitoring and Error Tracking: LangServe integrates with monitoring\\ntools, including LangSmith, to provide real-time insights into API perfor-\\nmance, usage metrics, and error rates. This monitoring helps developers\\nmaintain optimal performance and quickly resolve issues as they arise.\\nLangServe’s streamlined workflow ensures that developers can deploy robust\\nAPIs with minimal overhead, making it a practical choice for scaling LLM ap-\\nplications in production environments.\\n4.3 Integration with LangSmith and LangChain\\nLangServe integrates seamlessly with LangSmith, which offers observability fea-\\ntures like tracing, logging, and performance monitoring. Through LangSmith,\\nLangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,\\nenabling developers to optimize applications based on real-time data.\\nAdditionally, LangServe’s integration with LangChain allows developers to\\nleverage LangChain’s extensive library of tools, models, and connectors as part'),\n",
       "  0.016666666666666666),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 11.0, 'page_label': '12', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='12 Vasilios Mavroudis\\nof the API. LangChain workflows, tools, and chains can be directly exposed\\nvia LangServe endpoints, providing flexible API interactions and enhancing the\\nfunctionality of LLM applications [5].\\n5 Limitations and Criticisms\\nLangChain provides a versatile framework for the development of applications\\npowered by large language models (LLMs). However, several limitations warrant\\nattention, especially in the domains of complexity and security.\\n5.1 Complexity\\nLangChain’s modular architecture, while designed to simplify LLM-based ap-\\nplication development, can paradoxically increase complexity. Effective use of\\nLangChain often requires a nuanced understanding of its distinct components,\\nsuch as LangGraph and LangSmith, as well as familiarity with its API ecosys-\\ntem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-\\nedge of each module may present a barrier to new users, complicating onboarding\\nand initial implementation phases.\\n5.2 Security Concerns\\nGiven LangChain’s modular design and extensive reliance on external integra-\\ntions, security presents a notable challenge. Although LangChain incorporates a\\nrange of security measures, including fine-grained permission control and sand-\\nboxing, the complexity of securing LLM-based applications remains high, espe-\\ncially for applications managing sensitive data. Below, we outline several critical\\nsecurity risks associated with LangChain and explore strategies for risk mitiga-\\ntion.\\nRisksAssociatedwithExternalProviders Toenhancefunctionality,LangChain\\nintegrateswithnumerousexternalservices,suchasvectordatabases,APIproviders,\\nand cloud storage platforms. However, these integrations expose applications to\\nsecurity vulnerabilities:\\n– Data Exposure: Accessing external resources can inadvertently expose sen-\\nsitive data to third-party providers, a risk particularly relevant for applica-\\ntions handling personal or confidential information. Without stringent data\\nencryption and access control mechanisms, the potential for data leaks or\\nunauthorized access increases.\\n– Third-Party Dependency: Reliance on third-party services introduces de-\\npendencies on their security protocols. Any compromise within a provider’s\\ninfrastructurecouldaffectLangChainapplications,resultingindatabreaches\\nor service interruptions. This underscores the importance of thoroughly vet-\\nting providers and monitoring them for potential security issues.'),\n",
       "  0.015873015873015872),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 6.0, 'page_label': '7', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 7\\ncases, including inputs and expected outputs. Using these datasets, developers\\ncan conduct performance tests and assess how well their models meet expected\\noutcomes—an essential step for applications where accuracy and reliability are\\ncrucial. LangSmith supports custom evaluators, allowing developers to specify\\nscoring functions based on specific needs. For instance, an evaluator may mea-\\nsure the exact match between outputs and expected answers, or use metrics\\nlike cosine similarity for open-ended tasks. By supporting both built-in and cus-\\ntom evaluators, LangSmith provides flexibility in performance measurement for\\ndeterministic outputs or nuanced language generation tasks [9].\\n2.3 Dataset Management\\nDatasets are foundational to LangSmith’s evaluation system. Organizing test\\ncases into structured datasets enables methodical testing and validation. De-\\nvelopers can create datasets manually or import them from existing sources,\\nallowing diverse testing scenarios that reflect real-world use cases. Datasets can\\ncontain structured or unstructured data evaluations, accommodating a variety of\\ntesting needs. LangSmith’s dataset version control allows developers to maintain\\nmultiple dataset versions as applications evolve. This feature is critical for ensur-\\ning consistency in evaluation, especially as application logic changes or models\\nare retrained, providing a robust foundation for testing and validation [9].\\n2.4 LangSmith Workflow\\nLangSmith integrates tracing, evaluation, and dataset management into a cohe-\\nsive framework, enabling developers to progress from debugging to optimization\\nin a structured manner. A typical LangSmith workflow includes the following\\nstages:\\n– Trace Logging: Developers activate tracing on application functions, pro-\\nviding insights into model-component interactions.\\n– Dataset Creation and Evaluation: Developers create datasets represent-\\ning different scenarios to conduct comprehensive testing.\\n– Evaluation and Iterative Optimization: Evaluation results indicate per-\\nformance areas for refinement, guiding iterative application improvements.\\n– Version Control and Historical Tracking: LangSmith logs all interac-\\ntions, dataset versions, and evaluation scores, allowing developers to assess\\nimprovements over time.\\n2.5 Integration with LangChain and LangServe\\nLangSmith integrates seamlessly with LangChain and LangServe (Section 4) to\\nenhancetheend-to-endLLMapplicationdevelopmentexperience.ForLangChain\\nusers, LangSmith can automatically log traces and integrate with existing work-\\nflows. Combined with LangServe, LangSmith provides robust observability for\\nAPI deployments, monitoring real-time usage patterns, tracking request laten-\\ncies, and identifying bottlenecks.'),\n",
       "  0.015625)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this useful?\n",
    "- **Robustness**: Combines evidence from multiple ranking systems\n",
    "- **Smoothing**: The k parameter prevents individual high rankings from dominating\n",
    "- **Diversity**: Documents that appear high in multiple lists get higher final scores\n",
    "- **Normalization**: The reciprocal rank helps normalize scores across different ranking methods\n",
    "\n",
    "This is particularly valuable in RAG systems where you might have:\n",
    "- Different embedding models\n",
    "- Different retrieval strategies (semantic search, keyword search, etc.)\n",
    "- Different chunking strategies\n",
    "- Different similarity metrics\n",
    "\n",
    "By combining these different approaches, you often get better overall results than using any single method alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_retrieval_chain = generate_questions | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "question = \"How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?\"\n",
    "\n",
    "context = final_retrieval_chain.invoke(question)\n",
    "\n",
    "len(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific'),\n",
       "  0.06504494976203068),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'),\n",
       "  0.06378968253968254),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 5.0, 'page_label': '6', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='6 Vasilios Mavroudis\\n1.5 Advanced Components\\nBeyond these core elements, LangChain offers advanced modules that support\\ncomplex workflows, API deployments, and performance monitoring. These com-\\nponents are elaborated in the following sections:\\n– LangGraph for Stateful Process Modeling: Explored in Section 3,\\nLangGraph enables developers to structure applications with nodes and\\nedges, allowing for complex branching and multi-agent workflows.\\n– LangServe for API Deployment: Detailed in Section 4, LangServe facil-\\nitates the deployment of LangChain applications as REST APIs, supporting\\nscalability in production [5].\\n– LangSmith for Monitoring and Evaluation: Discussed in Section 2,\\nLangSmith offers tools for real-time performance monitoring, error tracking,\\nand version control to optimize applications iteratively [4].\\n2 LangSmith\\nLangSmith is a developer platform tailored to streamline the deployment, mon-\\nitoring, and evaluation of large language model (LLM) applications, provid-\\ning essential tools for building production-grade systems. Integrated with the\\nLangChain ecosystem, it enables users to trace, evaluate, and refine applications,\\nenhancing precision in complex environments. The platform addresses key chal-\\nlenges in observability, testing, and optimization, allowing developers to monitor\\nperformance, troubleshoot issues, and maintain high standards over time [9].\\n2.1 Tracing\\nTracing is a central feature of LangSmith, providing detailed visibility into how\\napplications interact with LLMs and external data sources. For developers using\\nLangChain, LangSmith offers tracing without the need for direct SDK inte-\\ngration, simplifying the monitoring process. Tracing involves capturing inputs,\\noutputs, and critical metadata from each interaction, allowing developers to ob-\\nserve and analyze component behavior in real time. This capability is especially\\nuseful for debugging and identifying bottlenecks within complex workflows.\\nLangSmith supports multiple ways to log traces, including languages like\\nPython and TypeScript. Each trace logs every call made to an LLM, along with\\ninput parameters, function annotations, and generated outputs, making it easier\\nto identify where adjustments may be necessary. By using traceable wrappers\\nand decorators, developers can annotate functions or data pipelines, enabling\\nautomatic trace logging with minimal code alterations [9].\\n2.2 Performance Testing\\nLangSmith’s evaluation tools enable developers to test and validate applica-\\ntions under real-world conditions. Evaluations require a defined dataset of test'),\n",
       "  0.047371031746031744),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 9.0, 'page_label': '10', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='10 Vasilios Mavroudis\\n3.4 Integration with LangChain and LangSmith\\nLangGraph integrates seamlessly with LangChain and LangSmith, although\\nit operates independently if desired. For users within the LangChain ecosys-\\ntem, LangGraph can utilize LangSmith’s tracing capabilities to monitor each\\nnode’s performance and capture detailed logs of agent interactions. Addition-\\nally, LangChain tools and APIs can be incorporated as graph nodes, expanding\\nLangGraph’s functionality with LangChain’s extensive library of tools and con-\\nnectors [7].\\n4 LangServe\\nLangServe [5] is an integral component of the LangChain ecosystem, specifically\\ndesignedtofacilitatethedeploymentoflargelanguagemodel(LLM)applications\\nas scalable REST APIs. With LangServe, developers can create production-\\ngrade APIs that allow external systems and users to interact with LangChain\\napplications. It enables LLM-powered applications to be served in real-time with\\nrobust support for load balancing, monitoring, and scalability.\\n4.1 Core Features of LangServe\\nAPI Deployment and Management LangServe simplifies the process of\\nturning LangChain applications into APIs, making LLM models accessible for\\nvarious services and client applications. With LangServe, any LangChain work-\\nflow can be packaged and exposed via RESTful endpoints, enabling interactions\\nwith language models, data retrieval, and external tool integration. The API-\\ncentric design allows LangServe to support diverse use cases, from chatbots and\\nrecommendation systems to complex multi-agent interactions. It provides several\\ntools for managing API endpoints, such as configurable routing, request han-\\ndling, and response formatting, which make it easy to customize each endpoint\\nbased on application requirements. This flexibility allows developers to design\\nAPIs with specific functionalities, making LangServe suitable for applications of\\nvarying complexity [5].\\nScalability and Load BalancingLangServe includes built-in support for scal-\\nability, making it ideal for applications that experience high traffic volumes. It\\ncan handle multiple API requests simultaneously, ensuring consistent perfor-\\nmance by balancing the load across instances. This feature is critical for pro-\\nduction environments where maintaining low response times under heavy loads\\nis essential. To further enhance scalability, LangServe provides tools for setting\\nup auto-scaling, which dynamically adjusts the number of instances based on\\ndemand. This allows applications to handle traffic spikes without degradation\\nin performance, making LangServe suitable for real-world deployments with un-\\npredictable usage patterns [5].'),\n",
       "  0.03333333333333333),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'),\n",
       "  0.03200204813108039),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 0.0, 'page_label': '1', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe,andLangSmith.Weexplorehowtheframeworkfacilitatesthe\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models· LLM Applications·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,'),\n",
       "  0.031754032258064516),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 12.0, 'page_label': '13', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 13\\nLangChain’s security model addresses many of these concerns, yet challenges\\npersist, particularly in sectors with rigorous compliance standards, such as fi-\\nnance and healthcare. Key areas for ongoing improvement include:\\n– DynamicPermissionAdjustment :CurrentpermissionsettingsinLangChain\\nare defined at deployment, but in dynamic applications, permissions may\\nneed to adapt based on user interactions. Implementing adaptive permis-\\nsions responsive to application state or user roles could enhance security.\\n– Advanced Encryption Standards: For applications processing highly\\nsensitive data, adopting advanced encryption practices—such as end-to-end\\nor field-level encryption—could bolster data security within even trusted\\nenvironments.\\n– Proactive Security Analytics: Integrating predictive analytics to pre-\\nemptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative\\nof potential breaches or misuse.\\nIn summary, LangChain’s security framework includes robust features such\\nas granular permissions, sandboxing, and real-time monitoring. While these mea-\\nsures provide a solid foundation, the ongoing challenge of securing LLM-driven\\napplications—particularly those relying on external providers—demands contin-\\nued advancements in security practices.\\n6 Conclusion\\nLangChain significantly advances the development of applications powered by\\nlarge language models (LLMs). Its modular framework—including components\\nlike LangGraph for stateful process modeling, LangServe for scalable API de-\\nployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-\\nments in usability and security, LangChain can continue to serve as a valuable\\ntool for developers seeking to leverage LLMs in building innovative and secure\\napplications. As industries increasingly adopt AI technologies, frameworks like\\nLangChain are poised to play a pivotal role in shaping the next generation of\\nintelligent, scalable, and secure solutions across various sectors.'),\n",
       "  0.016666666666666666),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 11.0, 'page_label': '12', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='12 Vasilios Mavroudis\\nof the API. LangChain workflows, tools, and chains can be directly exposed\\nvia LangServe endpoints, providing flexible API interactions and enhancing the\\nfunctionality of LLM applications [5].\\n5 Limitations and Criticisms\\nLangChain provides a versatile framework for the development of applications\\npowered by large language models (LLMs). However, several limitations warrant\\nattention, especially in the domains of complexity and security.\\n5.1 Complexity\\nLangChain’s modular architecture, while designed to simplify LLM-based ap-\\nplication development, can paradoxically increase complexity. Effective use of\\nLangChain often requires a nuanced understanding of its distinct components,\\nsuch as LangGraph and LangSmith, as well as familiarity with its API ecosys-\\ntem. Consequently, developers may face a steep learning curve, particularly those\\naiming for rapid prototyping or deployment. The need for comprehensive knowl-\\nedge of each module may present a barrier to new users, complicating onboarding\\nand initial implementation phases.\\n5.2 Security Concerns\\nGiven LangChain’s modular design and extensive reliance on external integra-\\ntions, security presents a notable challenge. Although LangChain incorporates a\\nrange of security measures, including fine-grained permission control and sand-\\nboxing, the complexity of securing LLM-based applications remains high, espe-\\ncially for applications managing sensitive data. Below, we outline several critical\\nsecurity risks associated with LangChain and explore strategies for risk mitiga-\\ntion.\\nRisksAssociatedwithExternalProviders Toenhancefunctionality,LangChain\\nintegrateswithnumerousexternalservices,suchasvectordatabases,APIproviders,\\nand cloud storage platforms. However, these integrations expose applications to\\nsecurity vulnerabilities:\\n– Data Exposure: Accessing external resources can inadvertently expose sen-\\nsitive data to third-party providers, a risk particularly relevant for applica-\\ntions handling personal or confidential information. Without stringent data\\nencryption and access control mechanisms, the potential for data leaks or\\nunauthorized access increases.\\n– Third-Party Dependency: Reliance on third-party services introduces de-\\npendencies on their security protocols. Any compromise within a provider’s\\ninfrastructurecouldaffectLangChainapplications,resultingindatabreaches\\nor service interruptions. This underscores the importance of thoroughly vet-\\nting providers and monitoring them for potential security issues.'),\n",
       "  0.01639344262295082),\n",
       " (Document(metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 10.0, 'page_label': '11', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': './data/langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 11\\nLatency and Error ManagementLangServe is designed to minimize latency,\\nensuring quick responses for each API request. It employs efficient request queu-\\ning and processing mechanisms to reduce waiting times. Additionally, LangServe\\nincludes error handling and retry logic, which helps maintain reliability by man-\\naging transient failures and minimizing downtime. This focus on latency and\\nerror resilience makes LangServe suitable for mission-critical applications that\\nrequire high availability. For instance, LangServe can automatically retry failed\\nrequests and log errors for further investigation, allowing developers to identify\\nissues promptly and maintain a stable API response rate [5].\\n4.2 LangServe Workflow\\nThe LangServe deployment workflow is straightforward, enabling developers to\\ngo from a LangChain application to a deployed API in a few steps. Here’s an\\noutline of a typical LangServe workflow:\\n1. Defining Endpoints: Developers define endpoints based on application re-\\nquirements, specifying which functions or models are exposed via API routes.\\nEach endpoint can have customized parameters, allowing for flexibility in\\nhow the API interacts with different components.\\n2. ConfiguringRequestHandlingandRouting :LangServeallowsforfine-\\ngrained control over how requests are processed. Developers can set up rout-\\ning rules, parameter validation, and request parsing to tailor the API expe-\\nrience.\\n3. Setting Up Load Balancing and Scaling: For applications with high\\ntraffic, LangServe’s load balancing can be configured to distribute requests\\nacross multiple instances, ensuring consistent response times. Auto-scaling\\ncan also be set up to dynamically adjust resources based on demand.\\n4. Monitoring and Error Tracking: LangServe integrates with monitoring\\ntools, including LangSmith, to provide real-time insights into API perfor-\\nmance, usage metrics, and error rates. This monitoring helps developers\\nmaintain optimal performance and quickly resolve issues as they arise.\\nLangServe’s streamlined workflow ensures that developers can deploy robust\\nAPIs with minimal overhead, making it a practical choice for scaling LLM ap-\\nplications in production environments.\\n4.3 Integration with LangSmith and LangChain\\nLangServe integrates seamlessly with LangSmith, which offers observability fea-\\ntures like tracing, logging, and performance monitoring. Through LangSmith,\\nLangServe users can track metrics such as request frequency, latency, and er-\\nror rates. This integration provides a comprehensive view of API performance,\\nenabling developers to optimize applications based on real-time data.\\nAdditionally, LangServe’s integration with LangChain allows developers to\\nleverage LangChain’s extensive library of tools, models, and connectors as part'),\n",
       "  0.01639344262295082)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def generate_response(question):\n",
    "    rag_system_prompt = \"\"\"\n",
    "        You are an AI language model assistant. Your task is to answer the user question based on the provided context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "    \"\"\"\n",
    "\n",
    "    rag_user_prompt = \"\"\"\n",
    "        Question: {question}\n",
    "    \"\"\"\n",
    "    context = final_retrieval_chain.invoke(question)\n",
    "\n",
    "    rag_system_message = SystemMessage(content=rag_system_prompt.format(context=context))\n",
    "    rag_user_message = HumanMessage(content=rag_user_prompt.format(question=question))\n",
    "\n",
    "    rag_prompt = [rag_system_message , rag_user_message]\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    response = llm.invoke(rag_prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain leverages its modular components—LangGraph, LangSmith, and LangServe—to effectively address the challenges of building scalable and secure applications powered by large language models (LLMs) in several ways:\n",
       "\n",
       "1. **LangGraph**:\n",
       "   - **Stateful Process Modeling**: LangGraph enables developers to structure applications using nodes and edges, facilitating complex branching and multi-agent workflows. This modularity allows for the creation of sophisticated applications that can manage state effectively, which is crucial for maintaining context in LLM interactions.\n",
       "   - **Integration with LangChain**: It can utilize LangSmith’s tracing capabilities to monitor performance and capture detailed logs of agent interactions, enhancing observability and debugging.\n",
       "\n",
       "2. **LangSmith**:\n",
       "   - **Monitoring and Evaluation**: LangSmith provides tools for real-time performance monitoring, error tracking, and version control, which are essential for optimizing applications iteratively. This helps developers maintain high standards and quickly address issues as they arise.\n",
       "   - **Tracing and Performance Testing**: It offers detailed visibility into how applications interact with LLMs and external data sources, allowing developers to log interactions and analyze performance. This capability is vital for identifying bottlenecks and ensuring that applications meet expected outcomes.\n",
       "   - **Dataset Management**: LangSmith supports the creation and management of datasets for testing, which is crucial for validating the performance of LLM applications under real-world conditions.\n",
       "\n",
       "3. **LangServe**:\n",
       "   - **API Deployment**: LangServe simplifies the process of deploying LangChain applications as scalable REST APIs, making LLM models accessible for various services. This API-centric design allows for flexible interactions and supports diverse use cases, from chatbots to complex multi-agent systems.\n",
       "   - **Scalability and Load Balancing**: It includes built-in support for scalability, enabling applications to handle high traffic volumes through load balancing and auto-scaling features. This ensures consistent performance and low response times, which are critical for production environments.\n",
       "   - **Latency and Error Management**: LangServe is designed to minimize latency and includes error handling mechanisms to maintain reliability, making it suitable for mission-critical applications.\n",
       "\n",
       "Overall, the modular architecture of LangChain allows developers to configure, extend, and deploy applications tailored to specific needs while addressing complexities related to scalability and security. By integrating these components, LangChain provides a comprehensive toolkit that enhances the development lifecycle of LLM applications, ensuring they are both efficient and secure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
