{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "## LLM\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "## Pinecone Vector Database\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"rag-setp-back-index\" # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from pprint import pprint\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Document (Uploading one file at a time)\n",
    "pdf_file_path = \"./data/langchain_turing.pdf\"\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Upload muiltiple PDF files from a directory\n",
    "# pdf_file_paths = <enter your path here>\n",
    "# loader = PyPDFDirectoryLoader(pdf_file_paths)\n",
    "\n",
    "# docs_dir = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=500)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index\n",
    "vectorstore = Pinecone.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
    "    index_name=index_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step_Back](./images/rag_step_back.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link -https://arxiv.org/pdf/2310.06117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class StepBackQuestion(BaseModel):\n",
    "    question: str= Field(description=\"The question derived by stepping back and reprahsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def step_back_function(question):\n",
    "    examples = [\n",
    "        {\n",
    "            \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "            \"output\": \"what can the members of The Police do?\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "            \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "        },\n",
    "    ]\n",
    "    # We now transform these to example messages\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"ai\", \"{output}\"),\n",
    "        ]\n",
    "    )\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples,\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "            ),\n",
    "            # Few shot examples\n",
    "            few_shot_prompt,\n",
    "            # New question\n",
    "            (\"user\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=1)\n",
    "\n",
    "    step_back_question = llm.with_structured_output(StepBackQuestion).invoke(prompt.format(question = question))\n",
    "\n",
    "    return step_back_question.question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What measures are taken to ensure security when integrating external services in applications?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How does LangChain ensure security when integrating external services like vector databases and API providers in LLM applications?\"\n",
    "step_back_question = step_back_function(question)\n",
    "step_back_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(question):\n",
    "\n",
    "    # Response prompt \n",
    "    response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "    # {normal_context}\n",
    "    # {step_back_context}\n",
    "\n",
    "    # Original Question: {question}\n",
    "    # Answer:\"\"\"\n",
    "\n",
    "    step_back_question = step_back_question = step_back_function(question)\n",
    "\n",
    "    normal_context = retriever.invoke(question)\n",
    "    step_back_context = retriever.invoke(step_back_question)\n",
    "    \n",
    "    llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=1)\n",
    "\n",
    "    response = llm.invoke(response_prompt_template.format(question = question, normal_context = normal_context, step_back_context = step_back_context))\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does LangChain ensure security when integrating external services like vector databases and API providers in LLM applications?\"\n",
    "\n",
    "answer = generate_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain addresses security concerns related to its integration with external services—such as vector databases and API providers—through a multi-faceted approach that includes several best practices and internal controls:\n",
       "\n",
       "1. **Granular Permissions**: LangChain adopts the principle of least privilege, allowing developers to set specific permissions for various components of their applications. This minimizes the risk of unauthorized access or actions within the application. By restricting access based on user roles and application context, it enhances security when interacting with external services.\n",
       "\n",
       "2. **Sandboxing**: To mitigate the risks associated with exposing sensitive data to external providers, LangChain employs sandboxed environments. This layered security measure helps isolate interactions, ensuring that even if a vulnerability occurs, the impact on the overall application and sensitive data is limited.\n",
       "\n",
       "3. **Data Encryption**: Although the document emphasizes the need for stringent encryption, it suggests that LangChain includes basic data security measures. For applications handling particularly sensitive information, it advocates for advanced encryption methodologies, such as end-to-end encryption and field-level encryption, to safeguard data even within trusted environments.\n",
       "\n",
       "4. **Auditability and Monitoring**: Using tools like LangSmith, LangChain provides detailed logging and monitoring capabilities. This feature allows developers to track application usage and detect anomalies in real-time, which is crucial for identifying potential security breaches early.\n",
       "\n",
       "5. **Proactive Security Analytics**: The framework can integrate predictive analytics to identify potential risks before they materialize. By analyzing application logs and monitoring for unusual patterns, machine learning models can flag anomalies indicative of security issues, enabling proactive responses to threats.\n",
       "\n",
       "6. **External Provider Vetting**: Given the reliance on third-party services, LangChain emphasizes the importance of thoroughly vetting these providers. This includes evaluating their security protocols and continuously monitoring their infrastructures to detect any vulnerabilities or breaches that could impact LangChain applications.\n",
       "\n",
       "Overall, while LangChain has established security measures to manage the complexities arising from external integrations, ongoing improvements—such as dynamic permission adjustments during application runtime and enhanced encryption practices—are suggested to bolster security further, especially in compliance-sensitive sectors like finance and healthcare."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
