{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "## LLM\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "## Pinecone Vector Database\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"rag-decomposition-index\" # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from pprint import pprint\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Document (Uploading one file at a time)\n",
    "pdf_file_path = \"./data/langchain_turing.pdf\"\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Upload muiltiple PDF files from a directory\n",
    "# pdf_file_paths = <enter your path here>\n",
    "# loader = PyPDFDirectoryLoader(pdf_file_paths)\n",
    "\n",
    "# docs_dir = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=500)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index\n",
    "vectorstore = Pinecone.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
    "    index_name=index_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link -\n",
    "- https://arxiv.org/pdf/2205.10625\n",
    "- https://arxiv.org/pdf/2212.10509"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Answer recursively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![REcursion](./images/rag_decompostion_recursion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Question(BaseModel):\n",
    "    generated_questions: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def generate_questions(question):\n",
    "    template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "    Generate multiple search queries related to: {question} \\n\n",
    "    Output (3 queries):\"\"\"\n",
    "    prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "    generated_questions_prompt = prompt_decomposition.invoke(\n",
    "        {\"question\": question}\n",
    "    )\n",
    "    llm_with_structured_output = llm.with_structured_output(Question)\n",
    "\n",
    "\n",
    "    generated_questions = llm_with_structured_output.invoke(generated_questions_prompt)\n",
    "\n",
    "    return generated_questions.generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are the key features of LangGraph and how do they contribute to building scalable LLM-powered applications?',\n",
       " 'How does LangSmith enhance the security of applications built using LangChain?',\n",
       " 'What role does LangServe play in the deployment of LLM-powered applications and how does it ensure scalability?']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?\"\n",
    "decompostion_questions = generate_questions(question)\n",
    "decompostion_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs if any:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\n",
    "Then based on the generated answer create a conhesive answer that draws from the responses of all the question + answer pairs and the answer to the latest question.\n",
    "\n",
    "Provide you answer:\n",
    "\"\"\"\n",
    "\n",
    "recursion_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rag(decompostion_questions):\n",
    "    \n",
    "    answer = \"\" \n",
    "    for q in decompostion_questions:\n",
    "        q_a_pairs = \"\"\n",
    "        context = retriever.invoke(q)\n",
    "\n",
    "        if q_a_pairs == \"\":\n",
    "            q_a_pairs = \"NO QA Pairs for now\"\n",
    "\n",
    "        answer = llm.invoke(recursion_prompt.invoke({\"context\" : context, \"question\" : q, \"q_a_pairs\" : q_a_pairs}))\n",
    "\n",
    "        q_a_pairs = format_qa_pair(q, answer.content)\n",
    "        q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pairs\n",
    "        \n",
    "    return answer.content\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieve_and_rag(decompostion_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The use of modular components in Large Language Model (LLM) application development offers several significant benefits:\n",
       "\n",
       "1. **Flexibility and Customization**: Modular components allow developers to tailor applications to specific requirements easily. This adaptability enables diverse workflows and functionalities, facilitating the integration of various tools as needed for specific tasks.\n",
       "\n",
       "2. **Simplified Development Cycle**: With a modular architecture, developers can focus on individual components without needing to understand the entire system at once. This approach simplifies the stages of development, productionization, and deployment, streamlining the development lifecycle.\n",
       "\n",
       "3. **Reusability**: Modules can be reused across different projects, reducing redundancy and fostering efficient resource management. This capability accelerates development timelines as developers can leverage pre-existing components instead of starting from scratch.\n",
       "\n",
       "4. **Scalability**: Modular designs generally support better scalability. Developers can add or replace components to meet increasing user demands or to incorporate new technologies without overhauling the entire application.\n",
       "\n",
       "5. **Focused Optimization and Monitoring**: Components like LangSmith enable detailed performance monitoring and evaluation of specific modules. This allows developers to track issues, optimize individual components, and improve the overall functionality of the application iteratively.\n",
       "\n",
       "6. **Enhanced Collaboration**: A modular approach allows teams to work on different components concurrently, facilitating collaboration among developers. This parallel development can lead to faster delivery of application updates and features.\n",
       "\n",
       "7. **Security Controls**: Modular components can introduce targeted security measures. For instance, specific modules can be designed to handle sensitive tasks with increased security protocols, thereby minimizing the risks associated with data exposure and third-party dependencies.\n",
       "\n",
       "Overall, the modular architecture, as exemplified by frameworks like LangChain, significantly enhances the ability of developers to create scalable, efficient, and contextually aware LLM applications while addressing the complexities of integration and security. \n",
       "\n",
       "In summary, while LangChain offers a comprehensive toolkit for LLM application development with its modular components, it also introduces certain complexities, such as a steep learning curve and security considerations related to external integrations. Thus, effective training and best practices are essential for developers to fully leverage the benefits of this modularity while mitigating potential challenges."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Answer Individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Answer Individually](./images/rag_decomposition_individual.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are the key features of LangGraph and how do they contribute to building scalable LLM-powered applications?',\n",
       " 'How does LangSmith enhance the security of applications built using LangChain?',\n",
       " 'What role does LangServe play in the deployment of LLM-powered applications and how does it ensure scalability?']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompostion_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rag(decompostion_questions):\n",
    "    rag_results = []\n",
    "\n",
    "    for q in decompostion_questions:\n",
    "        context = retriever.invoke(q)\n",
    "        \n",
    "        template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "        Context: {context}\n",
    "        Question: {question} \n",
    "        Answer: \n",
    "        \"\"\"\n",
    "\n",
    "        prompt_rag = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "        answer = llm.invoke(prompt_rag.invoke({\"context\" : context, \"question\" : q}))\n",
    "\n",
    "        rag_results.append({\"question\" : q, \"answer\" : answer.content})\n",
    "\n",
    "    return rag_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are the key features of LangGraph and how do they contribute to building scalable LLM-powered applications?',\n",
       "  'answer': 'LangGraph offers key features such as support for cycles and branching in workflows, built-in persistence for state management, and human-in-the-loop capabilities. These features enable developers to construct flexible, stateful applications that can handle complex interactions and maintain continuity across sessions. By facilitating iterative logic and real-time updates, LangGraph enhances the scalability and adaptability of LLM-powered applications.'},\n",
       " {'question': 'How does LangSmith enhance the security of applications built using LangChain?',\n",
       "  'answer': 'LangSmith enhances the security of applications built using LangChain by providing detailed logging and monitoring capabilities, which enable developers to track application usage and detect anomalies in real-time. It supports auditability and the principle of least privilege by allowing fine-grained permission controls, minimizing the risk of unauthorized actions. Additionally, it employs sandboxing and layered security measures to protect sensitive data and limit exposure to vulnerabilities.'},\n",
       " {'question': 'What role does LangServe play in the deployment of LLM-powered applications and how does it ensure scalability?',\n",
       "  'answer': 'LangServe facilitates the deployment of LLM-powered applications as scalable REST APIs, allowing for production-grade interactions with external systems. It ensures scalability by supporting load balancing, handling multiple API requests simultaneously, and providing auto-scaling features to adjust resources dynamically based on traffic demands. This makes LangServe suitable for high-demand production environments while maintaining consistent performance.'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retrieve_and_rag(decompostion_questions)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs(results):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        formatted_string += f\"\\n\\nQuestion {i}: {result['question']}\\n\\nAnswer {i}: {result['answer']}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "context = format_qa_pairs(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question 1: What are the key features of LangGraph and how do they contribute to building scalable LLM-powered applications?\n",
       "\n",
       "Answer 1: LangGraph offers key features such as support for cycles and branching in workflows, built-in persistence for state management, and human-in-the-loop capabilities. These features enable developers to construct flexible, stateful applications that can handle complex interactions and maintain continuity across sessions. By facilitating iterative logic and real-time updates, LangGraph enhances the scalability and adaptability of LLM-powered applications.\n",
       "\n",
       "\n",
       "\n",
       "Question 2: How does LangSmith enhance the security of applications built using LangChain?\n",
       "\n",
       "Answer 2: LangSmith enhances the security of applications built using LangChain by providing detailed logging and monitoring capabilities, which enable developers to track application usage and detect anomalies in real-time. It supports auditability and the principle of least privilege by allowing fine-grained permission controls, minimizing the risk of unauthorized actions. Additionally, it employs sandboxing and layered security measures to protect sensitive data and limit exposure to vulnerabilities.\n",
       "\n",
       "\n",
       "\n",
       "Question 3: What role does LangServe play in the deployment of LLM-powered applications and how does it ensure scalability?\n",
       "\n",
       "Answer 3: LangServe facilitates the deployment of LLM-powered applications as scalable REST APIs, allowing for production-grade interactions with external systems. It ensures scalability by supporting load balancing, handling multiple API requests simultaneously, and providing auto-scaling features to adjust resources dynamically based on traffic demands. This makes LangServe suitable for high-demand production environments while maintaining consistent performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "answer = llm.invoke(prompt.invoke({\"context\" : context, \"question\" : question}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain leverages modular components such as LangGraph, LangSmith, and LangServe to effectively address challenges in building scalable and secure LLM-powered applications through their distinct yet complementary functionalities.\n",
       "\n",
       "LangGraph enhances scalability by providing features such as support for cycles and branching in workflows, built-in persistence for state management, and human-in-the-loop capabilities. These features allow developers to create flexible and stateful applications capable of managing complex interactions and maintaining continuity across user sessions. By facilitating iterative logic and real-time updates, LangGraph ensures that applications can adapt and scale efficiently to meet varying user demands.\n",
       "\n",
       "LangSmith contributes to security by implementing robust logging and monitoring capabilities, enabling developers to track application usage and detect anomalies in real-time. Its support for auditability and fine-grained permission controls minimizes the risk of unauthorized actions, while sandboxing and layered security measures protect sensitive data and reduce exposure to vulnerabilities. This emphasis on security is crucial for maintaining user trust and compliance in increasingly data-sensitive environments.\n",
       "\n",
       "LangServe plays a vital role in the deployment aspect, converting LLM-powered applications into scalable REST APIs. It ensures high scalability by supporting load balancing and managing multiple API requests simultaneously, along with auto-scaling features that dynamically adjust resources based on traffic demands. This capability makes LangServe ideal for operating in high-demand production scenarios while providing consistent application performance.\n",
       "\n",
       "Together, these modular components of LangChain create a cohesive ecosystem that addresses the intertwined challenges of scalability and security, allowing developers to build robust LLM-powered applications that can handle complex functionalities and maintain high standards of data protection."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
